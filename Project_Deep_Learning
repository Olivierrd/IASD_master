# -*- coding: utf-8 -*-
"""[Olivier - rapport] DeepLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k15He1uvEndM08eMPhZ0aj6M8Gb4Xw-E
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pybind11
#!pip3 install --upgrade tensorflow
import pybind11
print(pybind11.get_include())
!pip install torch
import tensorflow as tf
from numpy.random import seed
seed(1)
import torch    
import tensorflow.keras as keras
from tensorflow.keras import layers 
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.models import load_model
import datetime
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD,Adam,Adamax,nadam,RMSprop
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D
from keras import regularizers
from keras.callbacks import ModelCheckpoint
!pip show tensorflow
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
# %cd "/content/drive/My Drive/Dauphine/Semestre 1/Deep_learning_CT/DeepLearningProject/"
!ls
!bash compile.sh

# create test sample
def create_test_train_db(db,size) :
  shape_ = db.shape[0]
  train = db[:int(shape_*size)]
  test = db[int(shape_*size):]
  return train,test

def load_data(input_data = np.load ('/content/drive/My Drive/Dauphine/Semestre 1/Deep_learning_CT/DeepLearningProject/input_data.npy'),
              policy = np.load('/content/drive/My Drive/Dauphine/Semestre 1/Deep_learning_CT/DeepLearningProject/policy.npy'),
              value = np.load('/content/drive/My Drive/Dauphine/Semestre 1/Deep_learning_CT/DeepLearningProject/value.npy'),
              end = np.load('/content/drive/My Drive/Dauphine/Semestre 1/Deep_learning_CT/DeepLearningProject/end.npy')) : 
  input_data_train, input_data_test = create_test_train_db(input_data, 0.99)
  policy_train, policy_test = create_test_train_db(policy, 0.99)
  value_train, value_test = create_test_train_db(value, 0.99)
  end_train, end_test = create_test_train_db(end, 0.99)
  return input_data_train, policy_train, value_train, input_data_test, policy_test, value_test

def dataset_(N =700000) :
  import golois
  planes = 8
  moves = 361
  dynamicBatch = True
  if dynamicBatch:
    import sys 
    sys.path.append('/content/drive/My Drive/Dauphine/Semestre 1/Deep_learning_CT/DeepLearningProject/golois.cpp')
    #import golois.cpp
    input_data = np.random.randint(2, size=(N, 19, 19, planes))
    input_data = input_data.astype ('float32')
        
    policy = np.random.randint(moves, size=(N,))
    policy = keras.utils.to_categorical (policy)
        
    value = np.random.randint(2, size=(N,))
    value = value.astype ('float32')
        
    end = np.random.randint(2, size=(N, 19, 19, 2))
    end = end.astype ('float32')
    golois.getBatch(input_data, policy, value,end)
    input_data_train, policy_train, value_train, input_data_test, policy_test, value_test  = load_data(input_data, policy, value,end)
    return input_data_train, policy_train, value_train, input_data_test, policy_test, value_test
input_data_train, policy_train, value_train, input_data_test, policy_test, value_test = dataset_(N =100000)

def generatorbis(N=100000):
  import golois
  import sys 
  planes = 8
  moves = 361
  epochs = 30
  sys.path.append('/content/drive/My Drive/Dauphine/Semestre 1/Deep_learning_CT/DeepLearningProject/golois.cpp')
  input_data = np.random.randint(2, size=(N, 19, 19, planes))
  input_data = input_data.astype('float32')
    
  policy = np.random.randint(moves, size=(N,))
  policy = keras.utils.to_categorical (policy)
    
  value = np.random.randint(2, size=(N,))
  value = value.astype('float32')
    
  end = np.random.randint(2, size=(N, 19, 19, 2))
  end = end.astype('float32')
  count=0
  while count<(98000000//N):
    golois.getBatch (input_data, policy, value, end)
    yield input_data,{'policy': policy, 'value': value}
    count+=1

def model_2(input, optimizer = keras.optimizers.SGD(lr=0.1)) :
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(input)
  ident = x
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)
  x = layers.add([ident,x])
  ident = x
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)
  x = layers.add([ident,x])
  ident = x
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)
  x = layers.Conv2D(32, 4, activation='relu', padding='same')(x)

  policy_head = layers.Conv2D(6, 4, activation='relu', padding='same')(x) 
  policy_head = layers.Flatten()(policy_head)
  policy_head = layers.Dense(361, activation='softplus', name='policy')(policy_head)

  value_head = layers.Flatten()(x)
  value_head = layers.Dense(1, activation='sigmoid', name='value')(value_head)
  return policy_head,value_head, optimizer

input = keras.Input(shape=(19, 19, 8), name='board')
policy_head, value_head,optimizer = model_2(input, optimizer = keras.optimizers.SGD(lr=0.1))
model = keras.Model(inputs=input, outputs=[policy_head, value_head])
model.compile(optimizer=optimizer,
                loss={'value': 'mse', 'policy': 'categorical_crossentropy'},
                metrics=['accuracy'])
history = model.fit(input_data_train, {'policy': policy_train, 'value': value_train},
           epochs=10, batch_size=123, validation_split=0.1)

# load and evaluate a saved model


model = load_model('model_MT_OR_v7 (1).h5')

for i in range(1,2) :
  print(datetime.datetime.now())
  input = keras.Input(shape=(19, 19, 8), name='board')
  if i > 1 : 
    save_name = 'weights-improvement_fffitgenerator_' + str(i) +'.h5'
    model.save(save_name)
    model = load_model(save_name)
  policy_head, value_head,optimizer = model_2(input, optimizer = keras.optimizers.SGD(lr=0.1/i))
  model.compile(optimizer=optimizer,
                loss={'value': 'mse', 'policy': 'categorical_crossentropy'},
                metrics=['accuracy'])
  model.summary()
  input_data,policy,value = load_data()[:3]
  history = model.fit_generator(generator=generatorbis(N=10000),
                  validation_data=(input_data,(policy,value)),
                  use_multiprocessing=None, 
                  steps_per_epoch = 2, 
                  epochs= 100, verbose=0)

keras.utils.plot_model(model, 'my_first_model_with_shape_info.png', show_shapes=True)

model = load_model('model_MT_OR_v7 (1).h5')
input = keras.Input(shape=(19, 19, 8), name='board')
input_data,policy,value = load_data()[:3]
model.evaluate(input_data, {'policy': policy, 'value': value})
