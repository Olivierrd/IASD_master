# -*- coding: utf-8 -*-
"""TP_appr_par_renfocement_[MT|OR].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VjPNc2JDKXYbY-AzU7WcNLOiVoWFZ6_G

Ce notebook regroupe les exercices dans la cadre de la note de TP pour le cours d'apprentissage par renforcement. 

Il a été réalisé par Maxime Talarmain et Olivier Randavel

1.   Bandits : implémentation des algorithmes du cours (epsilon-greedy, initialisation optimiste, intervalles de confiance, montée de gradient. Tracez les différentes courbes. 
  
  **Soumission:** le code pour générer les données et un documents avec les differentes courbes.
2.   Monde grille 4x3 du TD: implémentez les algorithmes itérations sur les politiques et itérations sur les valeurs. Utilisez r=-0.04 pour mettre au point vos algorithmes. 
  
  **Soumission**: le code et un document qui synthétise les différentes politiques optimales en fonction de la valeur de r (on peut imaginer qu’on a au moins ces trois différentes politiques optimales: si r est très négatif, on va chercher à écourter le plus possible l’épisode; si r a des valeurs intermédiaires, on va sûrement utiliser le plus court chemin vers l’état terminal qui donne +1; si r est grand, on va éviter de tomber dans un état terminal).

3. Monde grille 4x3 du TD: implémentez les algorithmes Monte Carlo "on policy" avec la version première et chaque visite. Pour une valeur de r fixée, comparez les deux algorithmes.

  **Soumission**: le code et un document qui synthétise les résultats.
Vous pouvez réaliser ce travail en binôme. N'oubliez pas d'indiquer vos deux noms.

Nous vous souhaitons bonne lecture. Pour toutes questions ou commentaires vous pouvez nous contacter par mail : [olivier.randavel@gmail.com](olivier.randavel@gmail.com) ou [maxime.talarmain@dauphine.eu](maxime.talarmain@dauphine.eu).

De plus il est possible de commenter directement ce notebook en utilisant le [lien](https://colab.research.google.com/drive/1VjPNc2JDKXYbY-AzU7WcNLOiVoWFZ6_G) vers le colab (fourni dans le mail) : vous trouverez une icone en haut à droite intitulée "Comment" (dans la version anglaise)

# **Libraries**
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
import scipy.stats as stats
import math
from random import uniform
from matplotlib import pyplot as plt

"""#**1. Bandits**

##**Get the best machine**
"""

def number_end(bestmachine):
  if bestmachine==2 : return "nd"
  elif bestmachine==1: return "st",
  elif bestmachine==3: return "rd"
  else : return "th"

k= np.random.normal(0, 1, 10)
bestmachine = np.where(k == np.amax(k))[0][0] + 1
print("the bestmachine is the",bestmachine,number_end(bestmachine))

"""## **Evaluation empirique : greedy, epsilon-greedy**"""

def tirage(time, epsilon) :
  y = np.zeros(1000)
  best= np.zeros(1000)
  for j in range (0,time) :
    recompense = []
    k= np.random.normal(0, 1, 10)
    bestmachine = np.where(k == np.amax(k))[0][0]
    bestmachine_array = []
    count = np.zeros(10)
    q = np.zeros(10)
    
    for i in range (1,1001):
      u1 = np.random.rand(1)      
      if u1[0]<= epsilon :
        number = np.random.randint(10)
        count[number] = count[number] + 1
        
      else : 
        number = np.where(q == np.amax(q))[0][0]
        count[number] = count[number] + 1
        
      r = np.random.normal(k[number], 1, 1)
      recompense.append(r[0])
      if bestmachine == number:
        bestmachine_array.append(1)
      else : 
        bestmachine_array.append(0)
      q[number] = q[number] + (1/count[number])*(r[0]-q[number])
    y = y + recompense
    best = best + bestmachine_array
  best = best/time  
  y = y/time
  return y , best

epsilon = [0, 0.1, 0.01]
time = 2000
for o in range (0,len(epsilon)) :
  data = tirage(time,epsilon[o])[0]
  label_name = 'epsilon = ' + str(epsilon[o])
  plt.plot(data, label = label_name)
plt.title('10-armed bandits -- average over 2000 runs' , loc='center')
plt.xlabel('itérations')
plt.ylabel('average reward')
plt.legend()
plt.show()

"""## **Fréquence du choix de la meilleure machine**"""

epsilon = [0, 0.1, 0.01]
time = 2000
for o in range (0,len(epsilon)) :
  data = tirage(time,epsilon[o])[1]
  label_name = 'epsilon = ' + str(epsilon[o])
  plt.plot(data, label = label_name)

plt.title('10-armed bandits -- average over 2000 runs' , loc='center')
plt.xlabel('itérations')
plt.ylabel('frequency of choosing the optimal action')
plt.legend()
plt.show()

"""## **Evaluation empirique**"""

def tirage_ee(time, alpha, type) :
  y = np.zeros(1000)
  best= np.zeros(1000)
  for j in range (0,time) :
    recompense = []
    k= np.random.normal(0, 1, 10)
    bestmachine = np.where(k == np.amax(k))[0][0]
    bestmachine_array = []
    count = np.zeros(10)
    if type=="Realistic" :
      epsilon = 0.1
      q = np.zeros(10)
    else :
      epsilon = 0
      q = np.ones(10)*5
    for i in range (1,1001):
      u1 = np.random.rand(1)      
      if u1[0]<= epsilon :
        number = np.random.randint(10)
        count[number] = count[number] + 1
        
      else : 
        number = np.where(q == np.amax(q))[0][0]
        count[number] = count[number] + 1
        
      r = np.random.normal(k[number], 1, 1)
      recompense.append(r[0])
      if bestmachine == number:
        bestmachine_array.append(1)
      else : 
        bestmachine_array.append(0)
      q[number] = q[number] + alpha*(r[0]-q[number])

    y = y + recompense
    best = best + bestmachine_array
  best = best/time  
  y = y/time
  return y , best

time = 2000
epsilon = [0.1, 0]
type = ["Realistic", "Optimitic"]
label = ["Realistic Q1 = 0, greedy epsilon = 0.1" , "Optimistic Q1 = 5, Pure greed epsilon = 0"]
for o in range(0,2) :
  data = tirage_ee(time, 0.1, type[o])[1]
  label_name = label[o]
  plt.plot(data, label = label_name)
plt.title('10-armed bandits -- average over 2000 runs, alpha = 0.1 fixed in updtae rule' , loc='center')
plt.xlabel('itérations')
plt.ylabel('frequency of choosing the optimal action')
plt.legend()
plt.show()

"""## **Utiliser un intervalle de confiance**"""

def tirage_ic(time,c) :
  y = np.zeros(1000)
  best= np.zeros(1000)
  for j in range (0,time) :
    recompense = []
    k= np.random.normal(0, 1, 10)
    bestmachine = np.where(k == np.amax(k))[0][0]
    bestmachine_array = []
    count = np.zeros(10)
    q = np.zeros(10)
    BS = np.zeros(10)
    for i in range (1,1001):
      if np.where(count == np.amin(count))[0][0] == 0 : 
         number = np.where(count == np.amin(count))[0][0] #obliger que toutes les machines soient utilisées au moins une fois 
      else : 
        number = np.where(BS == np.amax(BS))[0][0]       
      count[number] = count[number] + 1
      r = np.random.normal(k[number], 1, 1)
      recompense.append(r[0])
      if bestmachine == number:
        bestmachine_array.append(1)
      else : 
        bestmachine_array.append(0)
      q[number] = q[number] + 0.1*(r[0]-q[number])
      for h in range(0,10) : 
        BS[h] = q[h] + c*np.sqrt(np.log(i)/max(1,count[h]))
    y = y + recompense
    best = best + bestmachine_array
  best = best/time  
  y = y/time
  return y , best

time = 2000
type = ["Realistic"]

data = tirage_ee(time, 0.1, type[0])[1]
label_name = 'Realistic Q1 = 0, greedy epsilon = 0.1'
plt.plot(data, label = label_name)

data = tirage_ic(time,2)[1]
label_name = 'UCB c=2'
plt.plot(data, label = label_name)

data = tirage_ic(time,np.sqrt(2))[1]
label_name = 'UCB c=sqrt(2)'
plt.plot(data, label = label_name)

data = tirage_ic(time,np.sqrt(np.sqrt(2)))[1]
label_name = 'UCB c=sqrt(sqrt(2))'
plt.plot(data, label = label_name)

plt.title('10-armed bandits -- average over 2000 runs, alpha = 0.1 fixed in updtae rule' , loc='center')
plt.xlabel('itérations')
plt.ylabel('frequency of choosing the optimal action')
plt.legend()
plt.show()

"""**Observation :** Différentes valeurs de "c" ont été testées dans le but de trouver le même graphique que celui présenté dans le cours. On note que l'on s'y rapproche sans atteindre la même figure.

##**Montée de gradient stochastique**
"""

from random import uniform

def tirage_GS(time) :
  y = np.zeros(1000)
  best= np.zeros(1000)
  for j in range (0,time) :
    recompense = []
    k= np.random.normal(0, 1, 10)
    bestmachine = np.where(k == np.amax(k))[0][0]
    bestmachine_array = []
    count = np.zeros(10)
    B = np.zeros(10)
    pi = np.ones(10)*0.1
    for i in range (1,1001):
      u=np.random.uniform(0,1)
      if u<=pi[0]:
        number=0
      elif pi[0]<u and u<=pi[0]+pi[1]:  # we could use random choice 
        number=1
      elif pi[0]+pi[1]<u and u<=pi[0]+pi[1]+pi[2]:
        number=2
      elif pi[0]+pi[1]+pi[2]<u and u<=pi[0]+pi[1]+pi[2]+pi[3]:
        number=3
      elif pi[0]+pi[1]+pi[2]+pi[3]<u and u<=pi[0]+pi[1]+pi[2]+pi[3]+pi[4]:
        number=4
      elif pi[0]+pi[1]+pi[2]+pi[3]+pi[4]<u and u<=pi[0]+pi[1]+pi[2]+pi[3]+pi[4]+pi[5]:
        number=5
      elif pi[0]+pi[1]+pi[2]+pi[3]+pi[4]+pi[5]<u and u<=pi[0]+pi[1]+pi[2]+pi[3]+pi[4]+pi[5]+pi[6]:
        number=6
      elif pi[0]+pi[1]+pi[2]+pi[3]+pi[4]+pi[5]+pi[6]<u and u<=pi[0]+pi[1]+pi[2]+pi[3]+pi[4]+pi[5]+pi[6]+pi[7]:
        number=7
      elif pi[0]+pi[1]+pi[2]+pi[3]+pi[4]+pi[5]+pi[6]+pi[7]<u and u<=pi[0]+pi[1]+pi[2]+pi[3]+pi[4]+pi[5]+pi[6]+pi[7]+pi[8]:
        number=8
      else :
        number=9
      count[number] = count[number] + 1
      r = np.random.normal(k[number], 1, 1)
      recompense.append(r[0])
      if bestmachine == number:
        bestmachine_array.append(1)
      else : 
        bestmachine_array.append(0)
      for h in range(0,10) :
        if number == h: 
          B[h] = B[h] + 0.1*(r[0] - np.sum(recompense)/i)*(1- pi[h])
        else :
          B[h] = B[h] - 0.1*(r[0] - np.sum(recompense)/i)*pi[h]
      for hh in range(0,10) :
        pi[hh] = np.exp(B[hh])/np. sum(np.exp(B))
    y = y + recompense
    best = best + bestmachine_array
  best = best/time  
  y = y/time
  return y , best

count2 = 0
time = 2000
type = ["Realistic"]

data = tirage_ee(time, 0.1, type[0])[1]
label_name = 'Realistic Q1 = 0, greedy epsilon = 0.1'
plt.plot(data, label = label_name)

data = tirage_GS(time)[1]
label_name = 'Bandit Gradient'
plt.plot(data, label = label_name)



plt.title('10-armed bandits -- average over 2000 runs, alpha = 0.1 fixed in updtae rule' , loc='center')
plt.xlabel('itérations')
plt.ylabel('frequency of choosing the optimal action')
plt.legend()
plt.show()

"""# **2. Monde grille : TP**

###**Algorithme itératif  sur les valeurs :**
"""

#return la nouvelle position suivant la précédente
def haut(i,j) : 
  j=j+1
  return [i , j]
  
def bas(i,j) :
  j-=1
  return [i, j]
  
def gauche(i,j):
  i-=1
  return [i,j]
  
def droite(i,j):
  i+=1
  return [i,j]

def recompense(next_position,position,gain,r): 
  # return le gain mis à jour causé par le déplacement et la nouvelle position 
  # suivant : le gain précédent (gain), la position actuelle (position), la position souhaitée (next_position) et le gain des positions non terminales (r)
  end_good = [4,3] 
  end_bad = [4,2]  
  mur = [2,2]
  if (next_position == mur) or next_position[0] > 4 or  next_position[0] < 1 or next_position[1] > 3 or  next_position[1] < 1 :
    gain-= r
    return position, gain
  elif next_position == end_bad :
    gain-= 1
    return  next_position, gain
  elif next_position == end_good :
    gain+= 1
    return  next_position, gain
  else :
    gain-= r
    return next_position, gain

def proba_learning(i,j,d,r) : 
  # return le gains et les positions pour les 3 possibilités
  # suivant la position actuelle (i,j), la direction souhaitée (d), et la valeur du gain des positions non terminales (r)
  if d == 1 : # aller vers le haut
      position_gauche, gain_gauche = recompense(gauche(i,j),[i,j],0,r)
      position_droite,gain_droite = recompense(droite(i,j),[i,j],0,r)
      position_reussite, gain_reussite = recompense(haut(i,j),[i,j],0,r)
      return  position_reussite, gain_reussite, position_gauche, gain_gauche, position_droite, gain_droite

  elif d == 2 : # aller vers la gauche
      position_gauche, gain_gauche = recompense(bas(i,j),[i,j],0,r)
      position_droite,gain_droite = recompense(haut(i,j),[i,j],0,r)
      position_reussite, gain_reussite = recompense(gauche(i,j),[i,j],0,r)
      return  position_reussite, gain_reussite, position_gauche, gain_gauche, position_droite, gain_droite

  elif d == 3 : # aller vers la droite
      position_gauche, gain_gauche = recompense(haut(i,j),[i,j],0,r)
      position_droite,gain_droite = recompense(bas(i,j),[i,j],0,r)
      position_reussite, gain_reussite = recompense(droite(i,j),[i,j],0,r)
      return  position_reussite, gain_reussite, position_gauche, gain_gauche, position_droite, gain_droite
  
  else : # aller vers le bas
      position_gauche, gain_gauche = recompense(droite(i,j),[i,j],0,r)
      position_droite,gain_droite = recompense(gauche(i,j),[i,j],0,r)
      position_reussite, gain_reussite = recompense(bas(i,j),[i,j],0,r)
      return  position_reussite, gain_reussite, position_gauche, gain_gauche, position_droite, gain_droite


def converter(p) :
  # return suivant la position du mobile la valeur de v
  if p == [1,1] : return 0
  elif p == [1,2] : return 1
  elif p == [1,3] : return 2
  elif p == [2,1] : return 3
  elif p == [2,3] : return 5
  elif p == [3,1] : return 6
  elif p == [3,2] : return 7
  elif p == [3,3] : return 8
  elif p == [4,1] : return 9
  elif p == [4,2] : return 10
  elif p == [4,3] : return 11
  else : return "erreur"
  
All_position = [[1,1],[1,2],[1,3],[2,1],[2,3],[3,1],[3,2],[3,3],[4,1]]
Final_position = [[4,2],[4,3]]

def deplacement_learning(gamma,epsilon,r):
  v_new = np.zeros(12)
  delta = 10
  count = 0
  while delta >= epsilon :
      delta = 0
      for i in All_position :
        count+=1
        v_old = v_new[converter(i)]
        v_list = []
        H = proba_learning(i[0],i[1],1,r)
        G = proba_learning(i[0],i[1],2,r)
        D = proba_learning(i[0],i[1],3,r)
        B = proba_learning(i[0],i[1],4,r)
        RGauche = [H[3],G[3],D[3],B[3]]
        RDroite =   [H[5],G[5],D[5],B[5]]
        RReussite = [H[1],G[1],D[1],B[1]]
        Reussite = [H[0],G[0],D[0],B[0]]
        Gauche = [H[2],G[2],D[2],B[2]]
        Droite =  [H[4],G[4],D[4],B[4]] 
        for c in range(0,4) : #c = 0 --> haut , c = 1 --> Gauche, c = 2 --> Droite, c = 3 --> Bas
          v_list.append(0.8*(v_new[converter(Reussite[c])]*gamma+RReussite[c]) + 0.1*(v_new[converter(Gauche[c])]*gamma+RGauche[c]) + 0.1*(v_new[converter(Droite[c])]*gamma+RDroite[c]) )
        v_new[converter(i)] = max(v_list)
        delta = max(delta,abs(v_old-v_new[converter(i)]))
      for i in range(len(Final_position)) :
        v_old = v_new[converter(Final_position[i])]
        R = [-1,1]
        v_new[converter(Final_position[i])] = 1*(v_new[converter(Final_position[i])]*gamma+R[i])
        delta = max(delta,abs(v_old-v_new[converter(Final_position[i])]))
  return v_new,count


def path_(v) :
  # permet de simuler un chemin complet suivant v
  position = [1,1]
  path = []
  path.append(position)
  while position != [4,3] and position !=  [4,2] :
    i = position[0]
    j = position[1]
    n = []
   
    if converter(droite(i,j)) != "erreur" :
      n.append( v[converter(droite(i,j))] )
    else :
      n.append( v[converter(position)]) # si elle se prend un mur elle reste à sa place
    if converter(gauche(i,j)) != "erreur" :
      n.append( v[converter(gauche(i,j))] )
    else :
      n.append( v[converter(position)])
    if converter(haut(i,j)) != "erreur" :
      n.append( v[converter(haut(i,j))] )
    else :
      n.append( v[converter(position)])
    if converter(bas(i,j)) != "erreur" :
      n.append( v[converter(bas(i,j))] )
    else :
      n.append( v[converter(position)]) 

    bestway = np.where(n == np.amax(n))[0][0] 
    if bestway == 0 : 
      next_position = recompense(droite(i,j),position,0,0)[0]
    elif bestway == 1:
      next_position = recompense(gauche(i,j),position,0,0)[0]
    elif bestway == 2:
      next_position = recompense(haut(i,j),position,0,0)[0]
    else  :
      next_position = recompense(bas(i,j),position,0,0)[0]
    path.append(next_position)
    position = next_position
    if len(path) > 10 : #eviter les boucles infinies
      return path
  return path


  
def path_2(v,position,epsilon) :
    # permet d'obtenir la position suivante suivant la position actuelle 
    i = position[0]
    j = position[1]
    n = []
    if converter(droite(i,j)) != "erreur" :
      n.append( v[converter(droite(i,j))] )
    else :
      n.append( v[converter(position)]) # si elle se prend un mur elle reste à sa place
    if converter(gauche(i,j)) != "erreur" :
      n.append( v[converter(gauche(i,j))] )
    else :
      n.append( v[converter(position)])
    if converter(haut(i,j)) != "erreur" :
      n.append( v[converter(haut(i,j))] )
    else :
      n.append( v[converter(position)])
    if converter(bas(i,j)) != "erreur" :
      n.append( v[converter(bas(i,j))] )
    else :
      n.append( v[converter(position)]) 

    if epsilon < 0.1 : 
      bestway = np.random.randint(0,4)
    else : 
      bestway = np.where(n == np.amax(n))[0][0] 

    if bestway == 0 : 
      next_position, gain = recompense(droite(i,j),position,0,0)
    elif bestway == 1:
      next_position, gain = recompense(gauche(i,j),position,0,0)
    elif bestway == 2:
      next_position, gain = recompense(haut(i,j),position,0,0)
    else  :
      next_position, gain = recompense(bas(i,j),position,0,0)
    
    return next_position,gain

epsilon = 0.01
gamma_09 = 0.9

v1,count = deplacement_learning(gamma_09,epsilon,0.04)

All_positions = [[1,1],[1,2],[1,3],[2,1],[2,3],[3,1],[3,2],[3,3],[4,1],[4,2],[4,3]]

for i in All_positions :
   print("position_actuel : ",i, "position suivante : ", path_2(v1,i,epsilon = 0.4)[0])

print("")
print("gamma = 0.9 et r = -0.04 , v*:", v1)
print(path_(v1))

"""**Observations :** on obtient le v suivant 
v: [ 5.19356234  6.06733088  6.97710578  4.50962009  0.          8.12592205
  5.11452229  6.23847837  9.32175657  2.91733974 -9.91272036  9.91272036] pour r = -0.04 et un gamma = 0.9. Les valeurs correspondantes pour v sont à trouver dans la fonction converter.
"""

def seuil(gamma,epsilon):
  seuil = []
  best_path_1 = [[1,1],[2,1],[3,1],[3,2],[3,3],[4,3]]
  best_path_2 = [[1,1],[1,2],[1,3],[2,3],[3,3],[4,3]]
  for r in range(0,110) :
    path = path_(deplacement_learning(gamma,epsilon,r/100)[0])
    if path!=best_path_1 and path!=best_path_2 :
      seuil.append(r/100)
  for r in range(0,110) :
    path = path_(deplacement_learning(gamma,epsilon,-r/100)[0])
    if path!=best_path_1 and path!=best_path_2 :
      seuil.append(-r/100)
  return seuil

print("Le seuil est obtenu pour les valeurs de r suivantes (gamma = 0.9) : ", seuil(0.9, 0.01))

print("Le seuil est obtenu pour les valeurs de r suivantes (gamma = 0.5) : ", seuil(0.5, 0.01))

print("Le seuil est obtenu pour les valeurs de r suivantes (gamma = 0.1) : ", seuil(0.1, 0.01))

"""**Observations :** Lorsque l'on prend un gain négatif pour les stades non terminaux alors on obtient toujours le meilleur chemin. Neanmoins, lorsque le gain des états non terminaux est très proche de celui des états non terminaux alors on remarque un changement de chemin. 

En effet, plus le gamma est élevé plus on attache de l'importance aux derniers états et donc les seuils sont moins nombreux. Inversement lorsque gamma est faible.

###**Algorithme itératif ation de politique :**
"""

def deplacement_learning_2(times, gamma,epsilon,r):
  end_good = [4,3] 
  end_bad = [4,2]
  list_gain=[]
  path_length=[]
  v = np.zeros(12)
  GAIN = 0
  score=[]  
  best_gain = -1000
  manche_gagné = 0    #nombre de parties gagnées au cours des "times"
  for h in range (0,times) :
    start = [1,1]
    position = start
    gain = 0   #gain au début de la partie 
    lenght = 0 #nombre positions avant d'arriver à un état terminal
    path = []  #description du chemin emprunté à chaque position
    path.append(position)
    while position != end_bad and position != end_good  :
      i = position
      H = proba_learning(i[0],i[1],1,r)
      G = proba_learning(i[0],i[1],2,r)
      D = proba_learning(i[0],i[1],3,r)
      B = proba_learning(i[0],i[1],4,r)
      RGauche = [H[3],G[3],D[3],B[3]]
      RDroite =   [H[5],G[5],D[5],B[5]]
      RReussite = [H[1],G[1],D[1],B[1]]
      Reussite = [H[0],G[0],D[0],B[0]]
      Gauche = [H[2],G[2],D[2],B[2]]
      Droite =  [H[4],G[4],D[4],B[4]] 
      v_list = []
      for c in range(0,4) : #c = 0 --> haut , c = 1 --> Gauche, c = 2 --> Droite, c = 3 --> Bas
        v_list.append(0.8*(v[converter(Reussite[c])]*gamma+RReussite[c]) + 0.1*(v[converter(Gauche[c])]*gamma+RGauche[c]) + 0.1*(v[converter(Droite[c])]*gamma+RDroite[c]) )
      v[converter(position)] = max(v_list) 
      explorer = np.random.rand(1)[0]
      position, gain = path_2(v, position,explorer)
      path.append(position)
      lenght+=1     
    if position == end_bad :
      v[converter(end_bad)] = 1*(v[converter(end_bad)]*gamma-1)        
    else : 
      manche_gagné+=1 
      v[converter(end_good)] = 1*(v[converter(end_good)]*gamma+1)        
    if gain > best_gain :
      best_gain = gain
      best_path = path
    GAIN +=gain
    list_gain.append(GAIN)
    path_length.append(lenght)  
  score.append(100*manche_gagné/times)
  score = sum(score)/100
  return (list_gain,score,v,path_length)

times = 10000
epsilon = 0.1
gamma_09 = 0.9

(list_gain,score,v_algo2,path_length) = deplacement_learning_2(times, gamma_09,epsilon,-0.04)

All_positions = [[1,1],[1,2],[1,3],[2,1],[2,3],[3,1],[3,2],[3,3],[4,1],[4,2],[4,3]]

for i in All_positions :
  print("position_actuel : ",i, "position suivante : ", path_2(v_algo2,i,epsilon = 0.4)[0])

print("")
print("v : ",np.ndarray.tolist(v_algo2))

"""**Observations** :  Le v obtenu est assez similaire (voir paragraphe suivant) mais la complexité de calcul est plus importante.

### **Calcul Theorique de v comparaison avec les v calculés itérativements :**
"""

epsilon = 0.1
gamma_09 = 0.9

transition=np.array([[0.1,0.8,0,0.1,0,0,0,0,0,0,0],
[0,0.2,0.8,0,0,0,0,0,0,0,0],
[0,0.1,0.1,0,0.8,0,0,0,0,0,0],
[0,0,0,0.2,0,0.8,0,0,0,0,0],
[0,0,0,0,0.2,0,0,0.8,0,0,0],
[0,0,0,0.1,0,0,0.8,0,0.1,0,0],
[0,0,0,0,0,0,0.1,0.8,0,0.1,0],
[0,0,0,0,0,0,0.1,0.1,0,0,0.8],
[0,0,0,0,0,0.8,0,0,0.1,0.1,0],
[0,0,0,0,0,0,0,0,0,1,0],
[0,0,0,0,0,0,0,0,0,0,1]])
R_pi=np.ones(11)*(-0.04)
R_pi[9]=(-1)
R_pi[10]=1
gamma=0.9
vpi=np.linalg.inv(np.eye(11)-gamma*transition) @ R_pi

v = deplacement_learning(gamma_09,epsilon,0.04)
count = v[1]
v = np.ndarray.tolist(v[0][0:4]) + np.ndarray.tolist(v[0][5:])

print( "La distance entre la solution théorique et expériementale (itération sur les valeurs) est de " , np.linalg.norm(vpi - v), ". Cette valeur est faible." )
print("L'algoritme d'itérations sur les valeurs a fait ", count ," boucles avant de parvenir à l'obtention de v.")
v_iteratif = np.ndarray.tolist(v_algo2)[0:4]+  np.ndarray.tolist(v_algo2)[5:]

print( "La distance entre la solution théorique et expériementale (itération sur les politiques) est de " , np.linalg.norm(vpi - v_iteratif), ". Cette valeur est plus élevé que pour l'autre méthode itérative et a été obtenu après 10 000 politiques." )

"""**Observations :** Lors du calcul de v par itération de politique, le nombre d'itérations est très important pour parvenir à une bonne précision. En effet, apres 10 000 itérations la distance est supérieure à 2.5. Au contraire l'algo d'itération n'effectue qu'environ 300 itérations avec une précision inférieur à 2.

# **3. Monte Carlo**
"""

#retourne la direction (haut, gauche,droite, bas) souhaitée suivant la politique. 
def politique(pi,s):
  u = uniform(0,1)
  if u < pi[s][0] :
    return 1
  elif pi[s][0] <= u and u < pi[s][0] + pi[s][1] : 
    return 2
  elif  pi[s][0] + pi[s][1] <= u and u < pi[s][0] + pi[s][1] + pi[s][2]:
    return 3
  else :
    return 4

#retourne la direction finalement réalisée suivant les probabilités d'échouer (haut : 0.8, gauche,droite : 0.1)
def proba():
   u = uniform(0,1)
   if u <= 0.8 :
    return 0
   elif u > 0.8 and u <= 0.9 : 
    return 2
   elif u > 0.9 :
    return 4

def chaque_visite(epsilon) :
  q = np.zeros((12,4))
  q_next = np.ones((12,4))/4
  q_next[:][4] = 0
  n = np.zeros((12,4))
  Acc = np.zeros((12,4))
  pi = np.ones((12,4))/4
  end_good = [4,3] 
  end_bad = [4,2]
  gamma = 0.9
  epoch = 0
  while np.linalg.norm(q - q_next) > epsilon :
    epoch +=1
    position = [1,1]
    G = []
    path = []
    path.append(position)
    D = []
    while position != end_good and position != end_bad :
      d = politique(pi, converter(position))
      move = proba()
      temp = proba_learning(position[0],position[1],d,0.04)
      position = temp[move]
      gain = temp[move+1]
      G.append(gain)
      D.append(d-1)
      path.append(position)  
    d = politique(pi, converter(position))  
    D.append(d-1)
    for j in range(len(path)) : 
      q_next[converter(path[j]),D[j]] = q[converter(path[j]),D[j]]
      power = np.arange(0,len(path)-j) 
      power = gamma**power
      G_t = 0
      for jj in range(len(G)-j) :
       G_t +=  power[jj]*G[j+jj]
      if len(G)-j == 0 :
         G_t =  G[len(G)-1]
      Acc[converter(path[j]),D[j]] += G_t
      n[converter(path[j]),D[j]] += 1
      q[converter(path[j]),D[j]] = Acc[converter(path[j]),D[j]]/n[converter(path[j]),D[j]]
  
  for jjj in range(0,12) : 
   a_star = np.argmax(q[jjj][:])
   for k in range(4) :
     if a_star == k :  
       pi[jjj][k] = 1 - epsilon + epsilon/4
     else : 
       pi[jjj][k] = epsilon/4
  return pi,q,epoch

All_positions = [[1,1],[1,2],[1,3],[2,1],[2,3],[3,1],[3,2],[3,3],[4,1],[4,2],[4,3]]


pi_,q,epoch = chaque_visite(0.01)

for i in All_positions :
  temp = proba_learning(i[0],i[1],politique(pi_,converter(i)),0.04)
  print("position_actuel : ",i, "position suivante : ",temp[0]) 
print("")
print("pi_ :")
print(pi_)
print("")
print("q :")
print(q)
print("")
print("le nombre d'epochs est de :", epoch)

def premiere_visite(epsilon) :
  q = np.zeros((12,4))
  q_next = np.ones((12,4))/4
  q_next[:][4] = 0  
  n = np.zeros((12,4))
  Acc = np.zeros((12,4))
  pi = np.ones((12,4))/4
  end_good = [4,3] 
  end_bad = [4,2]
  gamma = 0.9
  epoch =0
  while np.linalg.norm(q - q_next) > epsilon :
    epoch +=1
    position = [1,1]
    G = []
    path = []
    path.append(position)
    D = []
    while position != end_good and position != end_bad :
      d = politique(pi, converter(position))
      move = proba()
      temp = proba_learning(position[0],position[1],d,0.04)
      position = temp[move]
      gain = temp[move+1]
      G.append(gain)
      D.append(d-1)
      path.append(position)  
    d = politique(pi, converter(position))  
    D.append(d-1)
    path_temp = []
    for j in range(len(path)) : 
      q_next[converter(path[j]),D[j]] = q[converter(path[j]),D[j]]
      if path[j] not in path_temp :
        power = np.arange(0,len(path)-j) 
        power = gamma**power
        G_t = 0
        for jj in range(len(G)-j) :
          G_t +=  power[jj]*G[j+jj]
        if len(G)-j == 0 :
          G_t =  G[len(G)-1]
        Acc[converter(path[j]),D[j]] += G_t
        n[converter(path[j]),D[j]] += 1
        q[converter(path[j]),D[j]] = Acc[converter(path[j]),D[j]]/n[converter(path[j]),D[j]]
  
  for jjj in range(0,12) : 
    a_star = np.argmax(q[jjj][:])
    for k in range(4) :
      if a_star == k :  
        pi[jjj][k] = 1 - epsilon + epsilon/4
      else : 
        pi[jjj][k] = epsilon/4
    #path_temp.append(path[jjj])
  return pi,q,epoch

All_positions = [[1,1],[1,2],[1,3],[2,1],[2,3],[3,1],[3,2],[3,3],[4,1],[4,2],[4,3]]

pi_,q, epoch = premiere_visite(0.01)

for i in All_positions :
  temp = proba_learning(i[0],i[1],politique(pi_,converter(i)),0.04)
  print("position_actuel : ",i, "position suivante : ",temp[0]) 
print("")
print("pi_ :")
print(pi_)
print("")
print("q :")
print(q)
print("")
print("nombre d'epochs :", epoch)

taux=0
for i in range(1000):
  pi_,q, epoch_premier = premiere_visite(0.01)
  pi_,q,epoch_chaque = chaque_visite(0.01)
  if epoch_premier>epoch_chaque :
    taux+=1
print('l algorithme de premiere visite est',taux,'plus rapide sur 1000 tentatives')
print('l algorithme de chaque visite est',1000-taux,'plus rapide sur 1000 tentatives')

premier=0
chaque=0
for i in range(1000):
  pi_,q, epoch_premier = premiere_visite(0.01)
  pi_,q,epoch_chaque = chaque_visite(0.01)
  premier+=epoch_premier
  chaque+=epoch_chaque
print('l algorithme de premiere visite converge en moyenne à',premier/1000,'epochs')
print('l algorithme de chaque visite converge en moyenne à',chaque/1000,'epochs')

"""**Observations** : On observe que dans le cadre de notre modélisation l'algorithme de chaque visite Monte Carlo On Policy converge plus rapidement en 325 epochs comparé à l'algorithme de premiere visite avec 418 epochs. Néanmoins,il faut prendre du recul sur ces chiffres puisque l'on travaille dans un univers non déterministe et donc si l'on rejoue l'algorithme plusieurs fois les résultats ne seront pas les mêmes. En moyenne, l'algorithme de chaque visite est légérement meilleur en terme de convergence mais la différence est négligeable (377 epochs VS 376 epochs)

La politique trouvée est semblable entre les deux algorithmes mais l'epsilon de  l'algorithme induit une erreur epsilon. Ainsi, la matrice π(S,A) n'a pas une politique déterministe comme cela était le cas pour la  modélisation précédente.
"""

