# -*- coding: utf-8 -*-
"""Explore_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Qt6TM6f95TPNlgxwXZuAxV_QdWJXVXH

#**Libraries**
"""

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

"""#**Explore dataset**"""

url = "https://raw.githubusercontent.com/datasets/covid-19/master/data/worldwide-aggregated.csv"
df = pd.read_csv(url, nrows = 20) #lecture de 20 lignes
df = pd.read_csv(url)

print(df.head(10), "\n")
print(df.describe(), "\n")
print(df.info(), "\n")
print(df.columns)
columns = df.columns.values

df = df.drop_duplicates(subset=None, keep='first', inplace=False)
df[columns[0]] = df[columns[0]].replace({'2020-':''}, regex=True)

print(df.info())

sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')

import seaborn as sns
df1 = df.drop(axis=0,columns=columns[0])
plt.figure(figsize=(16, 6))
sns.boxplot(x="variable", y="value", data=pd.melt(df1))
plt.show()

columns1 = df1.columns.values
for i in range(len(df1.columns)) : 
  plt.figure(figsize=(16, 6))
  data = df1[columns1[i]]
  sns.boxplot(x=data);

"""#**Visualisation**"""

#num_bins = df.count()[0]
plt.rcParams.update({'font.size': 16})

fig, ax1 = plt.subplots(figsize=(30,10))

colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']
for i in range(1,len(columns)-1) : 
  ax1.plot(df[columns[0]], df[columns[i]], color=colors[i], ls="-",label = columns[i] )

ax1.set_xlabel("date")
plt.xticks(rotation=45)
ax1.set_ylabel("number of people")
ax1.set_title("Evolution of the COVID19")
ax1.legend()
ax2 = ax1.twinx() 

ax2.plot(df[columns[0]], df[columns[len(columns)-1]], color=colors[len(columns)-1], ls="-",label = columns[len(columns)-1] )
ax2.set_ylabel("Pourcentage")
ax2.legend()

"""#**K-means**"""

# import data from url https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python#Mall_Customers.csv
from sklearn.preprocessing import MinMaxScaler

dataset = pd.read_csv("Mall_Customers.csv")
dataset.head()

data = dataset.drop(["CustomerID","Gender","Age"], axis = 1)
data.head()

sns.scatterplot(x="Annual Income (k$)", y="Spending Score (1-100)", data=data, s=30, color="red", alpha = 0.8)

# Menentukan variabel yang akan di klusterkan
data_x = dataset.iloc[:, 3:5]
data_x.head()
# Mengubah variabel data frame menjadi array
x_array =  np.array(data_x)
# Menstandarkan ukuran variabel
scaler = MinMaxScaler() #fungsinya untuk mengskalakan
x_scaled = scaler.fit_transform(x_array)
#Elbow method to minimize WSS (within-cluster Sum of Square)
Sum_of_squared_distances =[]
K = range(1,15)
for k in K:
    km =KMeans(n_clusters =k)
    km =km.fit(x_scaled)
    Sum_of_squared_distances.append(km.inertia_)
###plotting Elbow
plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

from yellowbrick.cluster import KElbowVisualizer

model = KMeans(random_state=123) 
# Instantiate the KElbowVisualizer with the number of clusters and the metric 
visualizer = KElbowVisualizer(model, k=(2,6), metric='silhouette', timings=False)
# Fit the data and visualize
visualizer.fit(x_scaled)    
visualizer.poof()

#5 is the best

kmeans.fit(x_scaled)
dataset["kluster"] = kmeans.labels_

# Memvisualkan hasil klusterplt.scatter(x_scaled[kmeans.labels_==0,0],x_scaled[kmeans.labels_==0,1],s=80,c='magenta',label='Careful')
plt.scatter(x_scaled[kmeans.labels_==1,0],x_scaled[kmeans.labels_==1,1],s=80,c='yellow',label='Standard')
plt.scatter(x_scaled[kmeans.labels_==2,0],x_scaled[kmeans.labels_==2,1],s=80,c='green',label='Target')
plt.scatter(x_scaled[kmeans.labels_==3,0],x_scaled[kmeans.labels_==3,1],s=80,c='cyan',label='Careless')
plt.scatter(x_scaled[kmeans.labels_==4,0],x_scaled[kmeans.labels_==4,1],s=80,c='burlywood',label='Sensible')
plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],marker = "o", alpha = 0.9,s=250,c='red',label='Centroids')
plt.title('Cluster of Clients')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show

"""#**Linear Regression**"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load DataSet
dataset = pd.read_csv("Mall_Customers.csv")
dataset["Gender"]=dataset["Gender"].replace("Male", 1)
dataset["Gender"]=dataset["Gender"].replace("Female", 0)
dataset.head

# Separate the independent variable (x) and dependent variable (y)
x = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 1].values
# Split the data into train and test part
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 0)
# Create the model
model = LinearRegression()
model.fit(xTrain, yTrain)
# Predict using test data
yPred = model.predict(xTest)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# generate random data-set
np.random.seed(0)
x = np.random.rand(100, 1)
y = 2 + 3 * x + np.random.rand(100, 1)

# sckit-learn implementation

# Model initialization
regression_model = LinearRegression()
# Fit the data(train the model)
regression_model.fit(x, y)
# Predict
y_predicted = regression_model.predict(x)

# model evaluation
rmse = mean_squared_error(y, y_predicted)
r2 = r2_score(y, y_predicted)

# printing values
print('Slope:' ,regression_model.coef_)
print('Intercept:', regression_model.intercept_)
print('Root mean squared error: ', rmse)
print('R2 score: ', r2)

# plotting values

# data points
plt.scatter(x, y, s=10)
plt.xlabel('x')
plt.ylabel('y')

# predicted values
plt.plot(x, y_predicted, color='r')
plt.show()

"""#**Logistic Regression**"""

#https://www.kaggle.com/c/titanic/data

train = pd.read_csv('train.csv')
train.head()

sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')

#boxplot with age on y-axis and Passenger class on x-axis.
plt.figure(figsize=(12, 7))
sns.boxplot(x='Pclass',y='Age',data=train,palette='winter')

def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    
    if pd.isnull(Age):
      if Pclass == 1:
              return 37
      elif Pclass == 2:
            return 29
      else:
              return 24
    else:
              return Age
train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')

train.drop("Cabin",axis=1,inplace=True)
train.dropna(inplace=True)
train.head()

sex = pd.get_dummies(train['Sex'],drop_first=True)
embark = pd.get_dummies(train['Embarked'],drop_first=True)
#drop the sex,embarked,name and tickets columns
train.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)
#concatenate new sex and embark column to our train dataframe
train = pd.concat([train,sex,embark],axis=1)
#check the head of dataframe
train.head()

#predicted if people survive
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1), 
           train['Survived'], test_size=0.30, 
            random_state=101)

from sklearn.linear_model import LogisticRegression
#create an instance and fit the model 
logmodel = LogisticRegression()
logmodel.fit(X_train, y_train)
predictions = logmodel.predict(X_test)
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, predictions))

# matrice de confusion
