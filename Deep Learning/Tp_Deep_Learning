# -*- coding: utf-8 -*-
"""D_TP_Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15eRCxfU69QQ_96IPRQf5B0tFR0FNjRSv

#**Library**
"""

import numpy as np
import pandas as pd
from tensorflow.keras import layers
from keras import regularizers 
import matplotlib.pyplot as plt
import random
from keras.datasets import mnist, reuters, boston_housing, cifar10
import matplotlib.pyplot 
from sklearn.preprocessing import StandardScaler
from keras.datasets import imdb
import tensorflow as tf
from keras.utils import to_categorical
from keras.models import Sequential
from tensorflow import keras
from sklearn.model_selection import train_test_split
from keras.optimizers import SGD, Nadam
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout
from google.colab import drive
from tensorflow.python.keras.layers.recurrent import LSTM


drive.mount('/content/drive', force_remount=True)

"""#**Cours 1**"""

# correction

i1 = 0.1
i2 = 0.5
eta = 1
out = 0.2
random.seed(5)
h=0
w1=np.random.uniform(0,1)
w2=np.random.uniform(0,1)
w3=np.random.uniform(0,1)
o=0

def sigmoid(h) :
  sigmoid = 1/(1+np.exp(-h))
  return sigmoid

error = 0.001
E=[]
O=[]
times = 0

while np.abs(out-o)/out >= error :



# for i in range(0,1000) :
  neth = w1 *i1 + w2*i2
  outh = sigmoid(neth)
  neto = w3*outh
  o = sigmoid(neto)
  err= 0.5*(o-outh)**2
  E.append(err)
  O.append(o)
#   print('err = ', err, 'o = ', o)
  dw3 = (o-out)*o*(1.0-o)*outh
  dw2 = (o-out)*o*(1.0-o)*w3*outh*(1.0-outh)*i2
  dw1 = (o-out)*o*(1.0-o)*w3*outh*(1.0-outh)*i1
  w3 = w3 -eta*dw3
  w2 = w2 -eta*dw2
  w1 = w1 -eta*dw1
  times +=1
  
plt.plot(O)
plt.plot(E)
print(times)

#Même exercice
model = Sequential()
model.add(Dense(7, input_dim=2))
model.add(Activation('sigmoid'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
sgd = SGD(lr=0.1)
model.compile(loss='binary_crossentropy',optimizer=sgd)
#on peut utiliser le mean_square ou le crossentroppy --> cost function that we want to minimize
model.fit(np.array([[0.1,0.5]]), np.array([[0.2]]), verbose=0, batch_size=1, epochs=100)
print(model.predict_proba(np.array([[0.1,0.5]])))

#le résultat est aléatoire ! Comment le faire toujours converger ? --> il faut utiliser plus de neuronne

# units nombre de neurones ici 1, et 16 entrées
model = Sequential()

model.add(Dense(16, input_shape=(2,)))
#on peut aussi prendre 8 à la place de 2
model.add(Activation('relu')) # on peut aussi utiliser la tanh
# why do we use tanh ? --> tanh put the results btw -1 and 1 
model.add(Dense(1))
model.add(Activation('sigmoid'))
# put the result between 0 and 1 
# when using softmax we obtain 1,1,1,1
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])


sgd = SGD(lr=0.1) 
# learning rate btw 0 and 1
model.compile(loss='binary_crossentropy',optimizer=sgd)
model.fit(X, y, verbose=0, batch_size=1,epochs=1000)
# epochs=1000 on effectue 1000 fois le backwards and forward
# verbose = 1 affichage des résltats au fur et à mesure 
# bach_size = 1  batch size defines the number of samples that will be propagated through the network
print(model.predict_proba(X))

"""# **Cours 2** 
Ce cours se base sur le livre e François Challet : Deep learnin with python
Création d'un classifieur d'image avce Keras enutilisant le data set Mnist
"""

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

print(test_labels[0:10])

matplotlib.pyplot.imshow(train_images[0])

"""On a des 60000 matrices 28x28 et on souhaite 60000 vecteurs de 28*28"""

print('max = ', train_images[1].max())
print('train_shape = ', train_images.shape)
print('test_shape = ', test_images.shape)

matrix_shape = train_images.shape[1]*train_images.shape[2]

grey_level = 255

train_images_01 = (train_images/grey_level)
test_images_01 = (test_images/grey_level)
print(train_images_01[1][0:50])

train_images_01_v = np.zeros((train_images.shape[0],matrix_shape))
test_images_01_v = np.zeros((test_images.shape[0],matrix_shape))
print(train_images_01_v.shape)

for i in range(0,train_images.shape[0]) :
  train_images_01_v[i] = train_images_01[i].reshape((1, 784))
for i in range(0,test_images.shape[0])  :
  test_images_01_v[i] = test_images_01[i].reshape((1, 784))
  
print(train_images_01_v[1][0:100])

#print('labels = ', train_labels)

label_test_v = np.zeros((test_labels.shape[0],10))
label_train_v = np.zeros((train_labels.shape[0],10))

for i in range (0,train_labels.shape[0]) :
  label_train_v[i][train_labels[i]] = 1
for i in range (0,test_labels.shape[0])  :
  label_test_v[i][test_labels[i]] = 1
  
  
# --> QUICKER
#label_test_v = to_categorical(test_labels)
#label_train_v = to_categorical(train_labels)


  
print(label_train_v[0:2])

"""An other way to transform value in a binelarly vector"""

# --> QUICKER
label_test_v2 = to_categorical(test_labels)
label_train_v2 = to_categorical(train_labels)

print(label_train_v2[0:10])
print(test_labels.shape)
print(test_labels[0:100])

"""Let's predict"""

# units nombre de neurones ici 1, et 16 entrées
model = Sequential()

model.add(Dense(512, input_shape=(784,)))
model.add(Activation('relu')) # on peut aussi utiliser la tanh

# model.add(Dense(28))
# model.add(Activation('relu'))

model.add(Dense(10))
model.add(Activation('softmax'))

y = label_train_v
X = train_images_01_v

print('input_shape : ', X.shape)
print('output_shape :', y.shape)
print("")



sgd = SGD(lr=0.1) 
# learning rate btw 0 and 1

#could also use sgb and categorical_crossentropy
model.compile(loss='binary_crossentropy',optimizer= 'rmsprop', metrics=['accuracy'])
#https://keras.io/losses/
model.fit(X, y, verbose=1, batch_size=128,epochs=2)

print(model.predict_proba(X))
print("")
print('test_accuracy = ',model.evaluate(test_images_01_v,label_test_v)[1])
print('loss =',model.evaluate(test_images_01_v,label_test_v)[0])

print(model.predict_classes(train_images_01_v[0:])[0])
print(matplotlib.pyplot.imshow(train_images[0]))

print(model.predict_classes(train_images_01_v[0:])[1])
print(matplotlib.pyplot.imshow(train_images[1]))

"""Remarque : 
> La loss baisse a chaque epoch et le taux d'erreur baisse aussi 

> Cela est dû au modèle qui apprend

> Attention : on identifie un surapprentissage si "acc" est très proche de 1

> A chaque epoch il modifie les poids 

> Les 60000 runs caractérisent l'app sur les 60000 images 

> proba donne la probabilité d'appartenir à une classe

##**Exercice : Sentimente analyse**

Le but est d'évaluer si un commentaire est positif ou négatif
"""

(train_data, train_labels) , (test_data,test_labels) = imdb.load_data(num_words=10000)
print('train_data_shape : ', train_data.shape)
print('train_label_shape : ', train_labels.shape)
print("")
print('train_data_sample : ', train_data[0:3])
print('train_label_sample : ', train_labels[0:10])

"""Label est constitué de 0 et 1 : 1 si le mot est présent dans le texte 
data est constitué d'id des 10000 mots : data[i]=124 et label[i]=1 donc le mot avec l'id 124 est présent dans le text.

Il faut changer comment les mots en mettant des 0 et un 1 donc des vecteurs de 10000
Pour chaque review il faut un vecteur donc un vecteur de 10000*reviews
"""

def vectorize_sequence(sequence, dimension=10000) :
  results = np.zeros((sequence.shape[0],10000))
  for j in range(0,sequence.shape[0]) :
    for i in range(0,len(sequence[j])) :
      results[j][sequence[j][i]] = 1
      
  return results

train_data_reshape=vectorize_sequence(train_data[:20000])
valid_data_reshape=vectorize_sequence(train_data[20000:])
test_data =vectorize_sequence(test_data)

y_train = np.array(train_labels[:20000]).astype('float32')
y_valid = np.array(train_labels[20000:]).astype('float32')

model=Sequential()
model.add(Dense(20, input_shape=(10000,)))
model.add(Activation('relu'))
model.add(Dropout(0.30)) #permet de supprimer 60% des neuronnes  donc on surapp moins
model.add(Dense(30,))
model.add(Activation('relu'))
model.add(Dropout(0.30)) #permet de supprimer 60% des neuronnes  donc on surapp moins
model.add(Dense(1))
model.add(Activation('sigmoid')) #une seul sortie entre 0 et 1

sgd = SGD(lr=0.1) 
# learning rate btw 0 and 1

#could also use adam/sgb/rmsprop and categorical_crossentropy
model.compile(loss='binary_crossentropy',optimizer= 'adam', metrics=['accuracy'])
#https://keras.io/losses/
do_plot_classique = model.fit(train_data_reshape, y_train, verbose=0, batch_size=512, epochs=10, validation_data=(valid_data_reshape, y_valid) )


print(model.predict_proba(train_data_reshape))
print("")
print('test_accuracy = ',model.evaluate(test_data,test_labels)[1])

print('loss =',model.evaluate(test_data,test_labels)[0])

"""Creating a plot representating the learning rate considering epochs in x axis and loss in y 

Same for accuracy with x epochs and y accuracy
"""

import matplotlib.pyplot as plt
#print(do_plot.history.keys())
loss_train = do_plot.history['loss']
loss_valid = do_plot.history['val_loss']

epochs = np.arange(len(loss_train))+1
plt.plot(epochs,loss_train, label='Training loss')
plt.plot(epochs,loss_valid, label='Validation loss')
plt.title = 'Training and validation loss'
plt.xlabel = 'epochs'
plt.ylabel = 'loss'
plt.legend()
plt.show()

acc_train = do_plot.history['acc']
acc_val= do_plot.history['val_acc']

epochs = np.arange(len(acc_train))+1
plt.plot(epochs,acc_train, label='Training acc')
plt.plot(epochs,acc_val, label='Validation acc')
plt.title = 'Training and validation accuracy'
plt.xlabel = 'epochs'
plt.ylabel = 'loss'
plt.legend()
plt.show()

"""#**Cours 3**

**Problème Multiclass**
"""

(train_data,train_labels) , (test_data,test_labels) = reuters.load_data(num_words = 10000)print(train_data.shape)
print(train_data)
print(train_labels.shape)
print(train_labels)

def sequence_2(sequence) :
  seq = np.zeros((sequence.shape[0],10000))
  for i in range(sequence.shape[0]) :
    for j in range(len(sequence[i])):
      seq[i][sequence[i][j]]+= 1
  return seq

train_data_reshape=sequence_2(train_data[:6000])
valid_data_reshape=sequence_2(train_data[6000:])
test_data_reshape=sequence_2(test_data[:6000])

#on met transpose car il nous faut les 10000 mots en ligne car il y a 5000 docs

label_train_v = np.zeros((train_labels.shape[0],46))
for i in range (0,train_labels.shape[0]) :
  label_train_v[i][train_labels[i]] = 1

label_test_v = np.zeros((test_labels.shape[0],46))
for i in range (0,test_labels.shape[0]) :
  label_test_v[i][test_labels[i]] = 1
 


y_test = np.array(label_test_v).astype('float32')
y_train = np.array(label_train_v[:6000]).astype('float32')
y_valid = np.array(label_train_v[6000:]).astype('float32')

print(train_data_reshape.shape)
print(y_train.shape)
print(y_valid.shape)
print(train_data_reshape)
print(y_train)

model=Sequential()
model.add(Dense(60, input_shape=(train_data_reshape.shape[1],)))
model.add(Activation('relu'))
model.add(Dropout(0.30)) #permet de supprimer 60% des neuronnes  donc on surapp moins
model.add(Dense(60))
model.add(Activation('relu'))
model.add(Dropout(0.30)) #permet de supprimer 60% des neuronnes  donc on surapp moins
model.add(Dense(46))
model.add(Activation('softmax')) #pas de sigmoid sinon on a de la classification binaire !!

sgd = SGD(lr=0.1) 
# learning rate btw 0 and 1

#could also use adam/sgb/rmsprop and categorical_crossentropy
model.compile(loss='categorical_crossentropy',optimizer= 'rmsprop', metrics=['accuracy'])
#https://keras.io/losses/
do_plot = model.fit(train_data_reshape, y_train, verbose=1, batch_size=50,epochs=8,
                    validation_data=(valid_data_reshape, y_valid) )

print(model.predict_proba(train_data_reshape))
print("")
print('test_accuracy = ',model.evaluate(test_data_reshape,y_test)[1])

print('loss =',model.evaluate(test_data_reshape,y_test)[0])

#il y a 46 classes en sortie donc il est imortant de garder bcp de neuronnes pour maximiser la variété 
#En multiclasse il faut toujours prendre catgorical sinon le calcul de l accn'est pas la valeur à optimiser

acc_train = do_plot.history['acc']
acc_val= do_plot.history['val_acc']

epochs = np.arange(len(acc_train))+1
plt.plot(epochs,acc_train, label='Training acc')
plt.plot(epochs,acc_val, label='Validation acc')
plt.title = 'Training and validation accuracy'
plt.xlabel = 'epochs'
plt.ylabel = 'loss'
plt.legend()
plt.show()

"""#**Cours 4 : La regression**

La caractéristique de cet exercice est que le nombre de données est faible.

Objectif : Estimer le prix de maisons suivant 13 caractéristiques
"""

(train_data,train_labels) , (test_data,test_labels) = boston_housing.load_data()

# print(train_data)
# print(train_labels)
# print(train_data.shape)
# print(train_labels.shape)
# print(test_data.shape)
# print(test_labels.shape)

"""Normalisation des données : il faut centrée et réduire les données --> E() = 0 et V() = 1. Mais il faut utiliser la moyenne et l ecart-type de X_train pour faire de meme avec X_test 
X - mean(X)
"""

mean = train_data.mean(axis = 0) # axis = 0 : moyenne sur la ligne
train_data -= mean
std = train_data.std(axis=0)
train_data/=std
test_data -=mean
test_data/=std


print(np.mean(train_data,axis=0))
print(np.std(train_data,axis=0,ddof=0))


#X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_labels, test_size=0.33, random_state=42)

print(y_train.shape)

model=Sequential()
model.add(Dense(64, input_shape=(train_data.shape[1],)))
model.add(Activation('relu'))
model.add(Dense(64))
model.add(Activation('relu'))
# model.add(Dropout(0.30)) #permet de supprimer 60% des neuronnes  donc on surapp moins
model.add(Dense(1))
model.add(Activation('linear')) #cette fonction d'activation est set par default,donc inutile
sgd = SGD(lr=0.1) 
# learning rate btw 0 and 1
model.compile(loss='mean_squared_error',optimizer= 'rmsprop', metrics=['mean_absolute_error','mean_absolute_percentage_error'])
print(model.fit(train_data, train_labels, verbose=0, batch_size=40,epochs=50, validation_split=0.33).history.get('val_mean_absolute_percentage_error')[49])
print("")

#mean_square_error car regression
model.predict_proba(train_data)
print(model.metrics_names)
print("")
print('all = ',model.evaluate(test_data,test_labels))

np.concatenate((X_1,X_2))

"""Utiliser la validation croisée pour obtenir un indicateur de la meilleur accuracy que l on peut avoir. Pour déterminer quand arreter les epochs (lorsque que cela overfitted) regarder la loss"""

score = []

model=Sequential()
model.add(Dense(64, input_shape=(train_data.shape[1],)))
model.add(Activation('relu'))
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('linear')) #cette fonction d'activation est set par default,donc inutile
sgd = SGD(lr=0.1) 
model.compile(loss='mean_squared_error',optimizer= 'rmsprop', metrics=['mean_absolute_error','mean_absolute_percentage_error'])

X_0 = train_data[int(train_data.shape[0]*0.33*0) : int(train_data.shape[0]*0.33*(1))]
X_1 = train_data[int(train_data.shape[0]*0.33*1) : int(train_data.shape[0]*0.33*(2))]
X_2 = train_data[int(train_data.shape[0]*0.33*2) : int(train_data.shape[0]*0.33*3)]

Y_0 = train_labels[int(train_labels.shape[0]*0.33*0) : int(train_labels.shape[0]*0.33*1)]
Y_1 = train_labels[int(train_labels.shape[0]*0.33*1) : int(train_labels.shape[0]*0.33*2)]
Y_2 = train_labels[int(train_labels.shape[0]*0.33*2) : int(train_labels.shape[0]*0.33*3)]

score.append(model.fit(np.concatenate((X_1,X_2)), np.concatenate((Y_1,Y_2)), verbose=0, batch_size=40,epochs=50, validation_data=(X_0,Y_0) ).history.get('val_mean_absolute_percentage_error')[49])
score.append(model.fit(np.concatenate((X_0,X_2)), np.concatenate((Y_0,Y_2)), verbose=0, batch_size=40,epochs=50, validation_data=(X_1,Y_1) ).history.get('val_mean_absolute_percentage_error')[49])
score.append(model.fit(np.concatenate((X_0,X_1)), np.concatenate((Y_0,Y_1)), verbose=0, batch_size=40,epochs=50, validation_data=(X_2,Y_2) ).history.get('val_mean_absolute_percentage_error')[49])
print(score, sum(score)/3, np.asarray(score).mean())

score = []
score_test =[]

score.append(model.fit(np.concatenate((X_1,X_2)), np.concatenate((Y_1,Y_2)), verbose=0, batch_size=40,epochs=50, validation_data=(X_0,Y_0) ).history.get('mean_absolute_error')[49])
score_test.append(model.evaluate(test_data,test_labels)[1])
score.append(model.fit(np.concatenate((X_0,X_2)), np.concatenate((Y_0,Y_2)), verbose=0, batch_size=40,epochs=50, validation_data=(X_1,Y_1) ).history.get('mean_absolute_error')[49])
score_test.append(model.evaluate(test_data,test_labels)[1])
score.append(model.fit(np.concatenate((X_0,X_1)), np.concatenate((Y_0,Y_1)), verbose=0, batch_size=40,epochs=50, validation_data=(X_2,Y_2) ).history.get('mean_absolute_error')[49])
score_test.append(model.evaluate(test_data,test_labels)[1])
print(score, sum(score)/3, np.asarray(score).mean())
print(score_test, sum(score_test)/3, np.asarray(score_test).mean())

"""We can use https://www.kaggle.com/shujunge/gridsearchcv-with-keras

#**Cours 5 :**

## **Using regularizer l2 :**
"""

(train_data, train_labels) , (test_data,test_labels) = imdb.load_data(num_words=10000)
print(train_data.shape)
print(train_data)
print(train_labels.shape)
print(train_labels)

def vectorize_sequence(sequence, dimension=10000) :
  results = np.zeros((sequence.shape[0],10000))
  for j in range(0,sequence.shape[0]) :
    for i in range(0,len(sequence[j])) :
      results[j][sequence[j][i]] = 1
      
  return results


test_data = vectorize_sequence(test_data)

train_data_reshape=vectorize_sequence(train_data[:20000])
valid_data_reshape=vectorize_sequence(train_data[20000:])

y_train = np.array(train_labels[:20000]).astype('float32')
y_valid = np.array(train_labels[20000:]).astype('float32')

model = Sequential()

model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation = 'relu', input_shape = (10000,)))  # le regulariser lambda est petit car c'est 1/lambda
model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))


sgd = SGD(lr=0.1) 
model.compile(loss='binary_crossentropy',optimizer= 'adam', metrics=['accuracy'])

do_plot1 = model.fit(train_data_reshape, y_train, verbose=0, batch_size=512, epochs=10, validation_data=(valid_data_reshape, y_valid) )
print(do_plot)
print("")

model.predict_proba(train_data_reshape)
print(model.metrics_names)
print("")
print('all = ',model.evaluate(test_data,test_labels))

"""L'accuracy de la méthode sans regularisation donne : test_accuracy =  0.867

Pour la méthode avec régularisation on a  test_accuracy =  0.85812
"""

import matplotlib.pyplot as plt
#print(do_plot.history.keys())
loss_train = do_plot1.history['loss']
loss_valid = do_plot1.history['val_loss']

epochs = np.arange(len(loss_train))+1
plt.plot(epochs,loss_train, label='Training loss')
plt.plot(epochs,loss_valid, label='Validation loss')
plt.title = 'Training and validation loss'
plt.xlabel = 'epochs'
plt.ylabel = 'loss'
plt.legend()
plt.show()

acc_train = do_plot1.history['acc']
acc_val= do_plot1.history['val_acc']

epochs = np.arange(len(acc_train))+1
plt.plot(epochs,acc_train, label='Training acc')
plt.plot(epochs,acc_val, label='Validation acc')
plt.title = 'Training and validation accuracy'
plt.xlabel = 'epochs'
plt.ylabel = 'loss'
plt.legend()
plt.show()

"""##**Using Dropout**"""

model = Sequential()

model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation = 'relu', input_shape = (10000,)))  # le regulariser lambda est petit car c'est 1/lambda
model.add(Dropout(0.50)) #permet de supprimer 70% des neuronnes  donc on surapp moins
model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'))
model.add(Dropout(0.50)) #permet de supprimer 70% des neuronnes  donc on surapp moins
model.add(Dense(1, activation = 'sigmoid'))


sgd = SGD(lr=0.1) 
model.compile(loss='binary_crossentropy',optimizer= 'adam', metrics=['accuracy'])

do_plot = model.fit(train_data_reshape, y_train, verbose=0, batch_size=512, epochs=10, validation_data=(valid_data_reshape, y_valid) )
print(do_plot)
print("")

model.predict_proba(train_data_reshape)
print(model.metrics_names)
print("")
print('all = ',model.evaluate(test_data,test_labels))

import matplotlib.pyplot as plt
loss_train = do_plot1.history['loss']
loss_valid = do_plot1.history['val_loss']


epochs = np.arange(len(loss_train))+1
plt.title = 'Training and validation loss'
plt.xlabel = 'epochs'
plt.ylabel = 'loss'
plt.figure(figsize = (10,10))


plt.plot(epochs,loss_train, label='Training loss regularizer')
plt.plot(epochs,loss_valid, label='Validation loss regularizer')

loss_train = do_plot.history['loss']
loss_valid = do_plot.history['val_loss']
plt.plot(epochs,loss_train, label='Training loss dropout')
plt.plot(epochs,loss_valid, label='Validation loss  dropout')


loss_train = do_plot_classique.history['loss']
loss_valid = do_plot_classique.history['val_loss']
plt.plot(epochs,loss_train, label='Training loss classique')
plt.plot(epochs,loss_valid, label='Validation loss  classique')


plt.legend()
plt.show()

"""On remarque que le validation dropout donne le plus faible résultat. Ce que l'on souhaite

##**Convolutional Networks** : 

ici on s'interesse au forme former par les couleurs
"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train.shape[0]

x_train = x_train.reshape(x_train.shape[0],28,28,1)
x_test = x_test.reshape(x_test.shape[0],28,28,1)
input_shape_ = (28,28,1)

x_train = x_train.astype("float32")
x_test = x_test.astype("float32")
x_train /=255
x_test /=255

num_classes = 10 # 10 chiffres de 0 à 9
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = Sequential()
model.add(Conv2D(32,kernel_size=(3,3), input_shape = input_shape_))
model.add(Activation('relu')) # on peut aussi utiliser la tanh
model.add(Conv2D(64,kernel_size=(3,3) ))
model.add(Activation('relu')) # on peut aussi utiliser la tanh
model.add(Flatten()) #réduire la dimension
model.add(Dense(128))
model.add(Activation('relu')) # on peut aussi utiliser la tanh
model.add(Dense(10))
model.add(Activation('softmax'))

sgd = SGD(lr=0.1) 
model.compile(loss='categorical_crossentropy',optimizer= 'adam', metrics=['accuracy'])
model.fit(x_train, y_train, verbose=1, validation_split=0.33, batch_size=128, epochs=2)

print(model.predict_proba(x_train))
print("")
print('test_accuracy = ',model.evaluate(x_test,y_test)[1])
print('loss =',model.evaluate(x_test,y_test)[0])

"""#**Cours 6 :**

##**Max Pooling**

Le but est de compresser (down_sample) une photo en prenant le max au niveau d'un filtre. 

Le but du **dropout** est de prendre un pourcentage des neuronnes et de les mettre à zéro. Cela permet de faire de la regularization et d'éviter l'overfitting
"""

print(y_train,x_train)
print(x_train.shape, y_train.shape)
print(x_train.shape[1], y_train.shape)

(x_train, y_train), (x_test, y_test) = mnist.load_data()
#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # chiffre entre zéro et neuf 
dim = 1

def Maxpoolin2D_(data, dim, type) :
  (x_train, y_train), (x_test, y_test) = data  
  x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[1],dim)
  x_test = x_test.reshape(x_test.shape[0],x_train.shape[1],x_train.shape[1],dim)
  input_shape_ = (x_train.shape[1],x_train.shape[1],dim)

  x_train = x_train.astype("float32")
  x_test = x_test.astype("float32")
  x_train /=255
  x_test /=255

  num_classes = 10 # 10 chiffres de 0 à 9
  y_train = keras.utils.to_categorical(y_train, num_classes)
  y_test = keras.utils.to_categorical(y_test, num_classes)
  model = Sequential()
  model.add(Conv2D(32,kernel_size=(3,3), activation='relu', input_shape = input_shape_))
  model.add(Dropout(0.2))
  model.add(Conv2D(64,kernel_size=(3,3), activation='relu' ))
  model.add(MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None))
  model.add(Conv2D(32,kernel_size=(3,3), activation='relu', input_shape = input_shape_))
  model.add(Dropout(0.2))
  model.add(Conv2D(64,kernel_size=(3,3), activation='relu' ))
  model.add(Flatten()) #réduire la dimension (matrice en vecteur)
  model.add(Dense(128, activation='relu'))
  if type == "Cifar":
      model.add(Dropout(0.2))
  model.add(Dense(10, activation='softmax'))

  sgd = SGD(lr=0.1) 
  model.compile(loss='categorical_crossentropy',optimizer= 'adam', metrics=['accuracy'])
  if type != "Cifar" :
    model.fit(x_train, y_train, verbose=1, validation_split=0.33, batch_size=128, epochs=2)
  else : 
    model.fit(x_train, y_train, verbose=1, validation_split=0.33, batch_size=128, epochs=60)
  print('test_accuracy = ',model.evaluate(x_test,y_test)[1])
  print('loss =',model.evaluate(x_test,y_test)[0])

Maxpoolin2D_(mnist.load_data(),1,"")

Maxpoolin2D_(keras.datasets.cifar10.load_data(),3, "Cifar")

"""## **The Functional API**"""

inputs = keras.Input(shape=(784,), name = 'img') # c'est un vecteur de 784 pas besoin de reshape
  x = layers.Dense(64,activation = 'relu')(inputs)
  x = layers.Dense(64,activation='relu')(x)
  outputs = layers.Dense(10,activation = 'softmax')(x)
  model = keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')
  keras.utils.plot_model(model,'my_first_model_with_shape_info.png',show_shapes = True)

def API_functional(data, dim, type) : 
  (x_train, y_train), (x_test, y_test) = data  
  print(y_train)
  x_train = x_train.reshape(x_train.shape[0],784)
  x_test = x_test.reshape(x_test.shape[0],784)
  input_shape_ = (x_train.shape[1],x_train.shape[1],dim)

  x_train = x_train.astype("float32")
  x_test = x_test.astype("float32")
  x_train /=255
  x_test /=255

  #num_classes = 10 # 10 chiffres de 0 à 9
  #y_train = keras.utils.to_categorical(y_train, num_classes)
  #y_test = keras.utils.to_categorical(y_test, num_classes)
  # car on a sparse_categorical_crossentropy
  model.compile(loss='sparse_categorical_crossentropy',  optimizer=keras.optimizers.RMSprop(),  metrics=['accuracy'])
  history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2)
  test_scores = model.evaluate(x_test, y_test, verbose=2)
  print('Test loss:', test_scores[0])
  print('Test accuracy:', test_scores[1])
  print(model.summary)

API_functional(mnist.load_data(),1,"")

"""#**Cours 7 :**

3 tensors en entrée et 2 tensors en sortie (Priorité du ticket et le departement : qui doit traiter le ticket)
"""

num_tags = 12
num_words = 10000
num_departments = 4

title_input = keras.Input(shape=(None,), name = "title")
body_input = keras.Input(shape=(None,), name = "body")
tags_input = keras.Input(shape=(num_tags,), name = "tags")

title_features = layers.Embedding(num_words, 64)(title_input)
title_features = layers.LSTM(128)(title_features)

body_features = layers.Embedding(num_words, 64)(body_input)
body_features = layers.LSTM(32)(body_features)

x = layers.concatenate([title_features, body_features, tags_input])
priority_pred = layers.Dense(1, activation='sigmoid', name='priority')(x)
department_pred = layers.Dense(num_departments, activation='softmax', name='department')(x) #ctivation crossentropy_sample_
model = keras.Model(inputs=[title_input, body_input, tags_input],outputs=[priority_pred, department_pred])
#model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=['mean_squared_error', 'categorical_crossentropy'], loss_weights=[1., 0.2])
model.compile(optimizer=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999), loss=['mean_squared_error', 'categorical_crossentropy'], loss_weights=[1., 0.2], metrics = ['acc','mse'])

#1*meansquare + 0.2*categorical_crossentropy

keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)

model.summary()

title_data = np.random.randint(num_words, size=(1280, 10))
body_data = np.random.randint(num_words, size=(1280, 100))
tags_data = np.random.randint(2, size=(1280, num_tags)).astype('float32')
# Dummy target data
priority_targets = np.random.random(size=(1280, 1))
dept_targets = np.random.randint(2, size=(1280, num_departments))

model.fit({'title': title_data, 'body': body_data, 'tags': tags_data},{'priority': priority_targets, 'department': dept_targets},epochs=10,batch_size=32)

"""##Exercice

pas de residuel (d addition ) apres un maxpoling car on a une reduction de la dimensionnalité
"""

inputs =  keras.Input(shape=(19,19,6), name = "input_1")

x = layers.(inputs)

policy = layers.Dense(361, activation='softmax', name='policy')(x)
value = layers.Dense(1, activation='sigmoid', name='value')(x)

"""##Alpha GO"""

def go(type = "residuel", type2 = "RMS") :
  input =layers.Input(shape =(32,32,3))
  x = layers.Conv2D(32,1,activation = 'relu', padding = "same")(input)
  ident = x
  x = layers.Conv2D(32,(3,3), activation = 'relu',padding = "same")(x)
  x = layers.Conv2D(32,(3,3), activation = 'relu',padding = "same")(x)
  if type == "residuel" :
    x = layers.add([ident,x]) #residuel avec same padding 
  x = layers.Flatten()(x)
  outputs = layers.Dense(10, activation = 'softmax')(x)
  model = tf.keras.models.Model(inputs=input, outputs=outputs)
  if type2 == "RMS" :
    model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss='sparse_categorical_crossentropy', metrics = ['acc'])
  else : 
    model.compile(optimizer=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999), loss='sparse_categorical_crossentropy', metrics = ['acc'])

  return model

keras.utils.plot_model(go(), 'multi_input_and_output_model.png', show_shapes=True)

keras.utils.plot_model(go('none','RMS'), 'multi_input_and_output_model.png', show_shapes=True)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # chiffre entre zéro et neuf 
x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],x_train.shape[3]).astype('float32')/255
x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],x_test.shape[3]).astype('float32')/255

model = go()
model.fit(x_train,y_train,epochs=10,batch_size=32)

model = go(type2 ="RMS")
model.fit(x_train,y_train,epochs=10,batch_size=32)



"""#**Cours 8 :**

##**LSTM** --> NLP / Time series
"""

data = pd.read_csv("/content/drive/My Drive/Dauphine/Deep_learning_CT/sinwave.csv")
data = np.reshape(np.array(data),(5000))
import torch

list = np.zeros((4950,50))
for i in range(4950) :
  list[i]=data[i:i+50]

len(list)
np.random.shuffle(list)
units = 50
train_data =list[:int(len(list)*0.90)]
test_data =list[int(len(list)*0.90):]

x_train = np.reshape(train_data, (train_data.shape[0],train_data.shape[1],1))
x_test = np.reshape(test_data, (test_data.shape[0],test_data.shape[1],1))

def LSTM() :
  model= Sequential()
  model.add(LSTM(50, input_shape=(50,1), return_sequences = True))
  model.add(Dropout(0.2))
  model.add(LSTM(100,return_sequences = False))
  model.add(Dropout(0.2))
  model.add(Dense(1))
  model.compile(loss="mse", optimizer = "rmsprop",  metrics = ['mae'])
  X_train,y_train, X_test, y_test = load_data("/content/drive/My Drive/Dauphine/Deep_learning_CT/sinwave.csv",50)
  model.fit(X_train,y_train, batch_size=512, nb_epoch=1, validation_split=0.05)
  predict = model.predict(X_test)
  return model

LSTM()
